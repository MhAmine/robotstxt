% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/get_robotstxts.R
\name{get_robotstxts}
\alias{get_robotstxts}
\title{function to get multiple robotstxt files}
\usage{
get_robotstxts(domain, warn = TRUE, force = FALSE, user_agent = NULL,
  ssl_verifypeer = c(1, 0), use_futures = FALSE)
}
\arguments{
\item{domain}{domain from which to download robots.txt file}

\item{warn}{warn about being unable to download domain/robots.txt because of}

\item{force}{if TRUE instead of using possible cached results the function will
re-download the robotstxt file
HTTP response status 404. If this happens,}

\item{user_agent}{HTTP user-agent string to be used to retrieve robots.txt file
from domain}

\item{use_futures}{Should future::future_lapply be used for possible
parallel/async retrieval or not. Note: check out help
pages and vignettes of package future on how to set up
plans for future execution because the robotstxt package
does not do it on its own.}
}
\description{
function to get multiple robotstxt files
}
